
# Chapter 11 -> Spark ETL with Apache Kafka

Task to do 
1. Create Apache Kafka Publisher, create the topic, and publish messages 
2. Create Apache Kafka Consumer and subscribe topic and receive messages
3. Create a Spark session & install the required libraries for Apache Kafka 
4. From the Spark, session subscribe earlier created topic 
5. Stream messages into the console 
6. Write streaming messages into the files (CSV or JSON or Delta format)
7. Write streaming messages to the database (MySQL or PostgreSQL or MongoDB)

Solution Notebook:<br/>
[Spark Notebook](chapter12.ipynb)

Blog with Explaination: <br/>
https://developershome.blog/2023/03/21/spark-etl-chapter-9-with-lakehouse-apache-iceberg/

YouTube video with Explanation: <br/>
https://www.youtube.com/watch?v=eL1xIjranhg

Meduim Blog Channel: <br/>
https://medium.com/@developershome
